{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import keras.utils\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a sequence in the following data format:\n",
    "* *x* and *y* describing a position in a grid of `100 x 100`\n",
    "* *c* describing a control status with 3 possible states (0 = starting, 1 = holding, 2 = pausing)\n",
    "\n",
    "State transitions follow this diagram:\n",
    "\n",
    "```\n",
    "+-+     +-+     +-+\n",
    "|0| --> |1| --> |2|\n",
    "+++  ^  +++  ^  +++\n",
    " ^   |   |   |   |\n",
    " |   |   |   |   |\n",
    " +---+---+   +---+\n",
    " |               |\n",
    " |               |\n",
    " +---------------+\n",
    "```\n",
    "\n",
    "This results in `100 x 100 x 3 = 30.000` possible one-hot encoded values ranging from 1 - 30.000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATES_COUNT = 3\n",
    "\n",
    "STATE_STARTING = 0\n",
    "STATE_HOLDING = 1\n",
    "STATE_PAUSING = 2\n",
    "\n",
    "DEFAULT_POSITION = [0, 0]\n",
    "\n",
    "\n",
    "def random_position(grid_size):\n",
    "    return [random.randint(0, grid_size - 1) for _ in range(2)]\n",
    "    \n",
    "\n",
    "def next_state(previous_state):\n",
    "    if previous_state == STATE_STARTING:\n",
    "        next_state = STATE_HOLDING\n",
    "    elif previous_state == STATE_HOLDING:\n",
    "        next_state = random.choice([\n",
    "            STATE_STARTING,\n",
    "            STATE_HOLDING,\n",
    "            STATE_PAUSING\n",
    "        ])\n",
    "    elif previous_state == STATE_PAUSING:\n",
    "        next_state = random.choice([\n",
    "            STATE_STARTING,\n",
    "            STATE_PAUSING\n",
    "        ])\n",
    "    else:\n",
    "        next_state = random.choice([\n",
    "            STATE_STARTING,\n",
    "            STATE_PAUSING\n",
    "        ])\n",
    "    return next_state\n",
    "    \n",
    "\n",
    "def generate_sequence(grid_size, seq_len):\n",
    "    sequence = []\n",
    "    current_state = None\n",
    "    current_position = DEFAULT_POSITION\n",
    "    for i in range(seq_len):\n",
    "        current_state = next_state(current_state)\n",
    "        if current_state == STATE_STARTING:\n",
    "            current_position = random_position(grid_size)\n",
    "        elif current_state == STATE_PAUSING:\n",
    "            current_position = DEFAULT_POSITION\n",
    "        feature_vector = np.concatenate([current_position, [current_state]])\n",
    "        sequence.append(feature_vector)\n",
    "    return sequence\n",
    "\n",
    "\n",
    "def generate_alternative_sequence(seq, grid_size):\n",
    "    sequence = generate_sequence(grid_size, len(seq))\n",
    "    current_position = None\n",
    "    for i in range(len(seq)):\n",
    "        if sequence[i][2] == STATE_STARTING:\n",
    "            # \"react\" to other sequence by flipping it\n",
    "            if seq[i][2] != STATE_PAUSING:\n",
    "                current_position = [seq[i][1], seq[i][0]]\n",
    "                sequence[i][0] = current_position[0]\n",
    "                sequence[i][1] = current_position[1]\n",
    "            else:\n",
    "                current_position= None\n",
    "        elif sequence[i][2] == STATE_HOLDING:\n",
    "            if current_position:\n",
    "                sequence[i][0] = current_position[0]\n",
    "                sequence[i][1] = current_position[1]\n",
    "    return sequence\n",
    "    \n",
    "\n",
    "def encode_sequence(seq, grid_size, pad=False):\n",
    "    # Encode 3-d vector in index values\n",
    "    m = np.zeros((grid_size, grid_size, STATES_COUNT))\n",
    "    indexed = [np.ravel_multi_index(vector, m.shape) + 1 for vector in seq]\n",
    "    if pad:\n",
    "        indexed = [0] + indexed[:-1]\n",
    "    n_tokens = (grid_size * grid_size * STATES_COUNT) + 1\n",
    "    hot_encoded = keras.utils.to_categorical(indexed, num_classes=n_tokens)\n",
    "    return hot_encoded\n",
    "\n",
    "\n",
    "def decode_sequence(seq, grid_size):\n",
    "    m = np.zeros((grid_size, grid_size, STATES_COUNT))\n",
    "    one_hot_decoded = [np.argmax(vector) for vector in seq]\n",
    "    decoded = [np.unravel_index((val - 1 if val > 0 else 0), m.shape) for val in one_hot_decoded]\n",
    "    return decoded\n",
    "\n",
    "\n",
    "def generate_dataset(grid_size, n_in, n_out, n_samples):\n",
    "    src_data, start_data, target_data = [], [], []\n",
    "    for i in range(n_samples):\n",
    "        # Generate source sequence\n",
    "        src = generate_sequence(grid_size, n_in)\n",
    "        src_encoded = encode_sequence(src, grid_size)\n",
    "        # Generate target sequence\n",
    "        target = generate_alternative_sequence(src[:n_out], grid_size)\n",
    "        target_encoded = encode_sequence(target, grid_size)\n",
    "        # Generated target input sequence, begin with start symbol 0\n",
    "        start_encoded = encode_sequence(target, grid_size, pad=True)\n",
    "        # ... add to dataset\n",
    "        src_data.append(src_encoded)\n",
    "        start_data.append(start_encoded)\n",
    "        target_data.append(target_encoded)\n",
    "        if i % 10000 == 0 and i > 0:\n",
    "            print(\"Generated sample no. #%d\" % i)\n",
    "    return np.array(src_data), np.array(start_data), np.array(target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATES_COUNT = 3\n",
    "\n",
    "def generate_sequence(length, n_unique):\n",
    "    return [random.randint(1, n_unique-1) for _ in range(length)]\n",
    "\n",
    "\n",
    "def generate_dataset(cardinality, n_in, n_out, n_samples):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    for _ in range(n_samples):\n",
    "        # generate source sequence\n",
    "        source = generate_sequence(n_in, cardinality)\n",
    "        # define target sequence\n",
    "        target = source[:n_out]\n",
    "        target.reverse()\n",
    "        # create padded input target sequence\n",
    "        target_in = [0] + target[:-1]\n",
    "        # encode\n",
    "        src_encoded = keras.utils.to_categorical([source], num_classes=cardinality)\n",
    "        tar_encoded = keras.utils.to_categorical([target], num_classes=cardinality)\n",
    "        tar2_encoded = keras.utils.to_categorical([target_in], num_classes=cardinality)\n",
    "        # store\n",
    "        X1.append(src_encoded)\n",
    "        X2.append(tar2_encoded)\n",
    "        y.append(tar_encoded)\n",
    "    return array(X1), array(X2), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_models(grid_size, latent_dim, n_tokens):\n",
    "    # Define an input sequence and process it.\n",
    "    encoder_inputs = Input(shape=(None, n_tokens))\n",
    "    encoder = LSTM(latent_dim,\n",
    "                   dropout=0.2,\n",
    "                   return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Set up the decoder, using `encoder_states` as initial state.\n",
    "    decoder_inputs = Input(shape=(None, n_tokens))\n",
    "    decoder_lstm = LSTM(latent_dim,\n",
    "                        return_sequences=True,\n",
    "                        return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(\n",
    "        decoder_inputs,\n",
    "        initial_state=encoder_states)\n",
    "    decoder_dense = Dense(n_tokens, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "    # Define sampling models\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    # Define inference decoder\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "    decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    # Return all models\n",
    "    return model, encoder_model, decoder_model\n",
    "\n",
    "\n",
    "def train(model, encoder_input, decoder_input, decoder_target, epochs, batch_size):\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    \n",
    "    history = model.fit([encoder_input, decoder_input], decoder_target,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_split=0.2)\n",
    "    return history\n",
    "\n",
    "\n",
    "def sample(encoder_model, decoder_model, input_seq, num_decoder_tokens, n_steps):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate an empty target sequence\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Sampling loop for a batch of sequences\n",
    "    sequence = []\n",
    "    for t in range(n_steps):\n",
    "        # Predict next vector\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "        # Store prediction to new sequence\n",
    "        sequence.append(output_tokens[0, 0, :])\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "        # Update target sequence\n",
    "        target_seq = output_tokens\n",
    "    return np.array(sequence)\n",
    "\n",
    "\n",
    "def evaluate(encoder_model, decoder_model, evaluation_total, n_tokens, n_in, n_out, grid_size):\n",
    "    evaluation_correct = 0\n",
    "    for _ in range(evaluation_total):\n",
    "        # Generate a test dataset\n",
    "        encoder_test, decoder_test, target_test = generate_dataset(\n",
    "            grid_size, n_in, n_out, 1)\n",
    "        # Sample some sequences with out trained model\n",
    "        target = sample(encoder_model, decoder_model, encoder_test, n_tokens, n_out)\n",
    "        if np.array_equal(\n",
    "            decode_sequence(target_test[0], grid_size),\n",
    "            decode_sequence(target, grid_size)):\n",
    "            evaluation_correct += 1\n",
    "    print(\"Accuracy: %.2f%%\" % (\n",
    "        float(evaluation_correct) / float(evaluation_total) * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 3 # How large is the grid of our x/y vectors\n",
    "\n",
    "n_in = 5 # Length of an input sequence\n",
    "n_out = 5 # Length of an output sequence\n",
    "\n",
    "n_dataset_samples = 100000 # Number of samples to train on\n",
    "latent_dim = 256 # Latent dimensionality of the encoding space\n",
    "batch_size = 512 # Batch size for training\n",
    "epochs = 10 # Number of epochs to train for\n",
    "\n",
    "n_evaluation = 100 # Number of samples to evaluate\n",
    "n_samples = 10 # Number of samples to generate as an example\n",
    "\n",
    "# All x/y positions in grid * state variants + 1 start symbol\n",
    "n_tokens = (grid_size * grid_size * STATES_COUNT) + 1\n",
    "\n",
    "# Print current configuration\n",
    "print(\"=======================================\")\n",
    "print(\"Number of dataset samples:\\t %d\" % n_dataset_samples)\n",
    "print(\"Number of unique tokens:\\t %d\" % n_tokens)\n",
    "print(\"_______________________________________\")\n",
    "print(\"Sequence length for inputs:\\t %d\" % n_in)\n",
    "print(\"Sequence length for outputs:\\t %d\" % n_out)\n",
    "print(\"_______________________________________\")\n",
    "print(\"Epochs:\\t\\t\\t\\t %d\" % epochs)\n",
    "print(\"Batch size:\\t\\t\\t %d\" % batch_size)\n",
    "print(\"Latent space dimension:\\t\\t %d\" % latent_dim)\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a simulated dataset\n",
    "encoder_input_data, decoder_input_data, decoder_target_data = generate_dataset(\n",
    "    grid_size, n_in, n_out, n_dataset_samples)\n",
    "\n",
    "print(\"Done! Shapes:\", encoder_input_data.shape,\n",
    "      decoder_input_data.shape,\n",
    "      decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_dataset(grid_size, n_in, n_out, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models\n",
    "model, encoder_model, decoder_model = define_models(\n",
    "    grid_size,\n",
    "    latent_dim,\n",
    "    n_tokens)\n",
    "\n",
    "# Train the model\n",
    "history = train(model,\n",
    "                encoder_input_data,\n",
    "                decoder_input_data,\n",
    "                decoder_target_data,\n",
    "                epochs,\n",
    "                batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "evaluate(encoder_model,\n",
    "         decoder_model,\n",
    "         n_evaluation,\n",
    "         n_tokens,\n",
    "         n_in,\n",
    "         n_out,\n",
    "         grid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a few examples\n",
    "for i in range(n_samples):\n",
    "    # Generate a sample dataset\n",
    "    encoder_test, decoder_test, target_test = generate_dataset(\n",
    "        grid_size,\n",
    "        n_in,\n",
    "        n_out,\n",
    "        1)\n",
    "    \n",
    "    # Sample some sequences with out trained model\n",
    "    target = sample(encoder_model,\n",
    "                    decoder_model,\n",
    "                    encoder_test,\n",
    "                    n_tokens,\n",
    "                    n_out)\n",
    "    \n",
    "    # Print it!\n",
    "    print('Sample #%i:\\nencoder_test=%s\\ntarget_test=%s\\ntarget=%s\\n' % (\n",
    "        i,\n",
    "        decode_sequence(encoder_test[0], grid_size),\n",
    "        decode_sequence(target_test[0], grid_size),\n",
    "        decode_sequence(target, grid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a plot\n",
    "acc = history.history[\"acc\"]\n",
    "val_acc = history.history[\"val_acc\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, \"bo\", label=\"Training acc\")\n",
    "plt.plot(epochs, val_acc, \"b\", label=\"Validation acc\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
