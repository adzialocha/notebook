{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from matplotlib import collections  as mc\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1337)\n",
    "np.random.seed(451)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_from_file(file):\n",
    "    # Read json sequence data\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SILENCE_CLASS = 0\n",
    "\n",
    "# Hyperparameters\n",
    "# ======================================================================\n",
    "batch_size = 128         # How many samples are in a batch\n",
    "seq_len = 10             # How long is the sequence / sample to train\n",
    "data_split = 1/3         # Percentage for validation & testing set\n",
    "\n",
    "num_layers = 0           # Number of hidden LSTM layers\n",
    "num_units = 32           # Number of units per LSTM layer\n",
    "epochs = 50              # How many epochs to train\n",
    "dropout = 0.2            # Dropout after every layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mono sequence data\n",
    "dataset = data_from_file('./files/mono-experiment/mono-sequence-original-6-998cls-2pca-6db-100ms.json')\n",
    "\n",
    "print(dataset['args'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset to our training sequence data\n",
    "sequence_data = []\n",
    "\n",
    "# Prepare dataset\n",
    "for step in dataset['steps']:\n",
    "    sound_event = dataset['events'][str(step['event_id'])]\n",
    "    \n",
    "    sound_class = sound_event['class']\n",
    "    \n",
    "    # Convert average RMS to dynamic class (0 - 9)\n",
    "    if sound_class is SILENCE_CLASS:\n",
    "        dynamic_class = 0\n",
    "    else:\n",
    "        dynamic_class = round(sound_event['rms_avg'] * 9)\n",
    "    \n",
    "    sequence_step = [\n",
    "        sound_class,\n",
    "        dynamic_class,\n",
    "    ]\n",
    "    \n",
    "    sequence_data.append(sequence_step)\n",
    "    \n",
    "n_sound_classes = dataset['args']['n_clusters'] + 2\n",
    "n_dynamic_classes = 10\n",
    "    \n",
    "# One hot encode it ..\n",
    "if False:\n",
    "    sound_class_hot = keras.utils.to_categorical(np.array(sequence_data)[:,0])\n",
    "    dynamic_class_hot = keras.utils.to_categorical(np.array(sequence_data)[:,1])\n",
    "\n",
    "    sequence_data = []\n",
    "    for x in range(len(sound_class_hot)):\n",
    "        # This is our data vector\n",
    "        sequence_data.append([\n",
    "            sound_class_hot[x],\n",
    "            dynamic_class_hot[x],\n",
    "        ])\n",
    "\n",
    "    n_sound_classes = len(sequence_data[0][0])\n",
    "    n_dynamic_classes = len(sequence_data[0][1])\n",
    "\n",
    "print('Sound classes: {}\\nDynamic classes: {}'.format(n_sound_classes, n_dynamic_classes))\n",
    "    \n",
    "sequence_data = np.array(sequence_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_data[11200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(data, seq_len, min_index, max_index):\n",
    "    i = min_index\n",
    "    while 1:\n",
    "        if i + batch_size >= max_index:\n",
    "            i = min_index\n",
    "        rows = np.arange(i, min(i + batch_size, max_index))\n",
    "        i += len(rows)\n",
    "        samples = np.zeros((len(rows), seq_len), dtype='int32')\n",
    "        targets = np.zeros((len(rows)), dtype='int32')\n",
    "        for j, _ in enumerate(rows):\n",
    "            indices = range(rows[j], rows[j] + seq_len)\n",
    "            if indices[-1] < max_index:\n",
    "                targets[j] = data[:, 0][indices][-1]\n",
    "                samples[j] = data[:, 0][indices]\n",
    "        yield samples, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in 3 sets for training, validation and testing\n",
    "validation_steps = round((data_split / 2) * len(sequence_data))\n",
    "\n",
    "train_max = len(sequence_data) - (validation_steps * 2)\n",
    "val_min = train_max + 1\n",
    "val_max = train_max + validation_steps + 1\n",
    "test_min = train_max + validation_steps + 2\n",
    "test_max = len(sequence_data) - 1\n",
    "\n",
    "training_steps = test_max - test_min\n",
    "\n",
    "train_gen = generator(sequence_data,\n",
    "                      seq_len=seq_len,\n",
    "                      min_index=0,\n",
    "                      max_index=train_max)\n",
    "\n",
    "val_gen = generator(sequence_data,\n",
    "                    seq_len=seq_len,\n",
    "                    min_index=val_min,\n",
    "                    max_index=val_max)\n",
    "\n",
    "test_gen = generator(sequence_data,\n",
    "                     seq_len=seq_len,\n",
    "                     min_index=test_min,\n",
    "                     max_index=test_max)\n",
    "\n",
    "steps_per_epoch = train_max // batch_size\n",
    "\n",
    "print('Batch size:', batch_size)\n",
    "print('Steps per epoch:', steps_per_epoch)\n",
    "\n",
    "print('\\nSplit for validation & test @ {0:.2f}%'.format(data_split * 100))\n",
    "print('Training set:', (0, train_max))\n",
    "print('Validation set:', (val_min, val_max))\n",
    "print('Test set:', (test_min, test_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=n_sound_classes,\n",
    "                           output_dim=num_units,\n",
    "                           input_length=seq_len))\n",
    "for n in range(num_layers - 1):\n",
    "    model.add(layers.LSTM(num_units, return_sequences=True))\n",
    "    if dropout > 0.0:\n",
    "        model.add(layers.Dropout(dropout))\n",
    "model.add(layers.LSTM(num_units))\n",
    "if dropout > 0.0:\n",
    "    model.add(layers.Dropout(dropout))\n",
    "model.add(layers.Dense(n_sound_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=steps_per_epoch,\n",
    "                              epochs=epochs,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot validation and training loss\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, acc, 'g', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
