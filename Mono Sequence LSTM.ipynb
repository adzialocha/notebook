{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from matplotlib import collections  as mc\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1337)\n",
    "np.random.seed(451)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_from_file(file):\n",
    "    # Read json sequence data\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "# ======================================================================\n",
    "dataset_path = './files/mono-experiment/tomomi-original-99cls-3pca-10db.json'\n",
    "use_dynamic_class = True\n",
    "use_duration_class = True\n",
    "\n",
    "# Hyperparameters\n",
    "# ======================================================================\n",
    "batch_size = 64          # How many samples are in a batch\n",
    "seq_len = 3              # How long is the sequence / sample to train\n",
    "data_split = 1/3         # Percentage for validation & testing set\n",
    "\n",
    "num_layers = 1           # Number of hidden LSTM layers\n",
    "num_units = 64           # Number of units per LSTM layer\n",
    "epochs = 100             # How many epochs to train\n",
    "dropout = 0.5            # Dropout after every layer\n",
    "\n",
    "# Model\n",
    "# ======================================================================\n",
    "model_path = './mono-sequence-original-5-198cls-2pca-6db-100ms.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mono sequence data\n",
    "dataset = data_from_file(dataset_path)\n",
    "\n",
    "print(dataset['args'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset to our training sequence data\n",
    "sequence_data = []\n",
    "\n",
    "# Prepare dataset\n",
    "for step in dataset['sequence']:\n",
    "    if not use_dynamic_class and not use_duration_class:\n",
    "        sequence_step = step['class_sound']\n",
    "    else:\n",
    "        sequence_step = [\n",
    "            step['class_sound'],\n",
    "        ]\n",
    "        \n",
    "        if use_dynamic_class:\n",
    "            sequence_step.append(step['class_dynamic'])\n",
    "            \n",
    "        if use_duration_class:\n",
    "            sequence_step.append(step['class_duration'])\n",
    "    \n",
    "    sequence_data.append(sequence_step)\n",
    "    \n",
    "n_sound_classes = dataset['args']['n_clusters'] + 1 # plus silence class\n",
    "n_dynamic_classes = dataset['args']['n_dynamics']\n",
    "n_duration_classes = dataset['args']['n_durations']\n",
    "\n",
    "if not use_dynamic_class and not use_duration_class:\n",
    "    feature_matrix = np.zeros((n_sound_classes))\n",
    "elif use_dynamic_class and not use_duration_class:\n",
    "    feature_matrix = np.zeros((n_sound_classes,\n",
    "                               n_dynamic_classes))\n",
    "elif not use_dynamic_class and use_duration_class:\n",
    "    feature_matrix = np.zeros((n_sound_classes,\n",
    "                               n_duration_classes))\n",
    "else: \n",
    "    feature_matrix = np.zeros((n_sound_classes,\n",
    "                               n_dynamic_classes,\n",
    "                               n_duration_classes))\n",
    "\n",
    "# Encode sequence\n",
    "if use_dynamic_class or use_duration_class:\n",
    "    sequence_data = [np.ravel_multi_index(v, feature_matrix.shape) for v in sequence_data]\n",
    "sequence_data = np.array(sequence_data)\n",
    "\n",
    "print('Total classes: {}, steps: {}'.format(np.max(sequence_data), len(sequence_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(data, seq_len, min_index, max_index):\n",
    "    i = min_index\n",
    "    while 1:\n",
    "        if i + batch_size >= max_index:\n",
    "            i = min_index\n",
    "        rows = np.arange(i, min(i + batch_size, max_index))\n",
    "        i += len(rows)\n",
    "        samples = np.zeros((len(rows), seq_len), dtype='int32')\n",
    "        targets = np.zeros((len(rows)), dtype='int32')\n",
    "        for j, _ in enumerate(rows):\n",
    "            indices = range(rows[j], rows[j] + seq_len)\n",
    "            if indices[-1] < max_index:\n",
    "                targets[j] = data[indices][-1]\n",
    "                samples[j] = data[indices]\n",
    "        yield samples, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in 3 sets for training, validation and testing\n",
    "validation_steps = round((data_split / 2) * len(sequence_data))\n",
    "\n",
    "train_max = len(sequence_data) - (validation_steps * 2)\n",
    "val_min = train_max + 1\n",
    "val_max = train_max + validation_steps + 1\n",
    "test_min = train_max + validation_steps + 2\n",
    "test_max = len(sequence_data) - 1\n",
    "\n",
    "training_steps = test_max - test_min\n",
    "\n",
    "train_gen = generator(sequence_data,\n",
    "                      seq_len=seq_len,\n",
    "                      min_index=0,\n",
    "                      max_index=train_max)\n",
    "\n",
    "val_gen = generator(sequence_data,\n",
    "                    seq_len=seq_len,\n",
    "                    min_index=val_min,\n",
    "                    max_index=val_max)\n",
    "\n",
    "test_gen = generator(sequence_data,\n",
    "                     seq_len=seq_len,\n",
    "                     min_index=test_min,\n",
    "                     max_index=test_max)\n",
    "\n",
    "steps_per_epoch = train_max // batch_size\n",
    "\n",
    "print('Batch size:', batch_size)\n",
    "print('Steps per epoch:', steps_per_epoch)\n",
    "\n",
    "print('\\nSplit for validation & test @ {0:.2f}%'.format(data_split * 100))\n",
    "print('Training set:', (0, train_max))\n",
    "print('Validation set:', (val_min, val_max))\n",
    "print('Test set:', (test_min, test_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = np.max(sequence_data) + 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=n_classes,\n",
    "                           output_dim=num_units,\n",
    "                           input_length=seq_len))\n",
    "for n in range(num_layers - 1):\n",
    "    model.add(layers.LSTM(num_units, return_sequences=True))\n",
    "    if dropout > 0.0:\n",
    "        model.add(layers.Dropout(dropout))\n",
    "model.add(layers.LSTM(num_units))\n",
    "if dropout > 0.0:\n",
    "    model.add(layers.Dropout(dropout))\n",
    "model.add(layers.Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=steps_per_epoch,\n",
    "                              epochs=epochs,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot validation and training loss\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, acc, 'g', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_temperature = 1.0\n",
    "\n",
    "# ~~~~~~\n",
    "\n",
    "def reweight_distribution(original_distribution, temperature):\n",
    "    distribution = np.log(original_distribution) / temperature\n",
    "    distribution = np.exp(distribution)\n",
    "    return distribution / np.sum(distribution)\n",
    "\n",
    "positive_results = 0\n",
    "total_results = 0\n",
    "\n",
    "for test_start in range(len(sequence_data) - seq_len):\n",
    "    slice_from = test_start\n",
    "    slice_to = test_start + seq_len\n",
    "\n",
    "    test_sequence = sequence_data[slice_from:slice_to]\n",
    "    test_target_class = sequence_data[slice_to:slice_to + 1][0]\n",
    "\n",
    "    test_result = model.predict(np.array([test_sequence]))\n",
    "\n",
    "    test_result_reweighted = reweight_distribution(test_result,\n",
    "                                                   test_temperature)\n",
    "    test_result_class = np.argmax(test_result_reweighted)\n",
    "    \n",
    "    if test_result_class == test_target_class:\n",
    "        positive_results += 1\n",
    "        \n",
    "    total_results += 1\n",
    "\n",
    "ratio = positive_results / total_results\n",
    "print('Finished test with {0} positives out of {1} (score={2:.0f}%)'.format(positive_results,\n",
    "                                                                            total_results,\n",
    "                                                                            ratio * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
